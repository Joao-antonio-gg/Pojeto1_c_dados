{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 1 - Ci√™ncia dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nome: \n",
    "\n",
    "Nome: \n",
    "\n",
    "    Nome: Mateus Ruggero SUAVIZA√á√ÉO DE LAPLACE N√ÉO ESQUECE\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aten√ß√£o: Ser√£o permitidos grupos de tr√™s pessoas, mas com uma rubrica mais exigente. Grupos deste tamanho precisar√£o fazer um question√°rio de avalia√ß√£o de trabalho em equipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Carregando algumas bibliotecas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esperamos trabalhar no diret√≥rio\n",
      "C:\\Users\\Andre\\Desktop\\Pojeto1_c_dados-main\n"
     ]
    }
   ],
   "source": [
    "print('Esperamos trabalhar no diret√≥rio')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando a base de dados com os tweets classificados como relevantes e n√£o relevantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Shang Chi.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Treinamento</th>\n",
       "      <th>Relevancia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>maldito capitalismo que n√£o me deixa ir ver sh...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>üí≠üí≠ shang chi usando fones de ouvido üí≠üí≠</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>queria assistir shang chi</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uma blusinha de shang chi pf renner riachuelo ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>indo assistir shang chi sozinha espero que ngn...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Treinamento  Relevancia\n",
       "0  maldito capitalismo que n√£o me deixa ir ver sh...           3\n",
       "1             üí≠üí≠ shang chi usando fones de ouvido üí≠üí≠           0\n",
       "2                          queria assistir shang chi           3\n",
       "3  uma blusinha de shang chi pf renner riachuelo ...           1\n",
       "4  indo assistir shang chi sozinha espero que ngn...           3"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_excel(filename)\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Teste</th>\n",
       "      <th>Relevancia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>esperando um drive de shang chi decente</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t√¥ falando: v√£o ver shang chi ü§© https://t.co/u...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>assisti shang - chi e pra mim √© o melhor filme...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sabe quando vc quer fazer uma coisa q sabe q e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>n tem uma m√≠sera alma pra ir ver shang chi com...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Teste  Relevancia\n",
       "0            esperando um drive de shang chi decente           1\n",
       "1  t√¥ falando: v√£o ver shang chi ü§© https://t.co/u...           1\n",
       "2  assisti shang - chi e pra mim √© o melhor filme...           2\n",
       "3  sabe quando vc quer fazer uma coisa q sabe q e...           1\n",
       "4  n tem uma m√≠sera alma pra ir ver shang chi com...           1"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_excel(filename, sheet_name = 'Teste')\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Classificador autom√°tico de sentimento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fa√ßa aqui uma descri√ß√£o do seu produto e o que considerou como relevante ou n√£o relevante na classifica√ß√£o dos tweets.\n",
    "\n",
    "O produto escolhido foi o filme Shang-Chi e a lenda dos dez an√©is.\n",
    "Para avaliar como relevante, bastava o usuario ter visto o filme e mostrado interesse ou querer ver de novo, mas para ser muito relevante era preciso passar alguma cr√≠tica ao filme ou mostrar muito entusiasmo a respeito."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Montando um Classificador Naive-Bayes\n",
    "\n",
    "Considerando apenas as mensagens da planilha Treinamento, ensine  seu classificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "\n",
    "def cleanup(text):\n",
    "    \"\"\"\n",
    "        Fun√ß√£o de limpeza muito simples que troca alguns sinais b√°sicos por espa√ßos\n",
    "    \"\"\"\n",
    "    #import string\n",
    "    punctuation = '[!-.:?;]' # Note que os sinais [] s√£o delimitadores de um conjunto.\n",
    "    pattern = re.compile(punctuation)\n",
    "    text_subbed = re.sub(pattern, '', text)\n",
    "    return text_subbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Transformando o total das palavras em DataFrame##############################################################################\n",
    "\n",
    "total_raw = [] #Colocando frases em uma lista\n",
    "for i in range(0, len(train)):\n",
    "    total_raw.append(train['Treinamento'][i])\n",
    "\n",
    "frases_lower = [] #Colocando as frases em lower case\n",
    "for i in range(0, len(total_raw)):\n",
    "    frases_lower.append(cleanup(total_raw[i].lower()))\n",
    "\n",
    "total = [] #Transformando a lista em um √≠ndice para cada palavra\n",
    "for frase in frases_lower:\n",
    "    x = frase.split()\n",
    "    for i in range(0, len(x)):\n",
    "        total.append(x[i])\n",
    "\n",
    "#Data Frame\n",
    "total_serie = pd.Series(total)\n",
    "total_relativo = total_serie.value_counts(True)\n",
    "\n",
    "###Transformando os tweets irrelevantes em DataFrame############################################################################\n",
    "\n",
    "irr_raw = [] #Colocando em uma lista\n",
    "tweets_irrelevantes = train.loc[train['Relevancia'] == 0, :]\n",
    "\n",
    "tweets_irrelevantes\n",
    "for tweet in tweets_irrelevantes['Treinamento']:\n",
    "    irr_raw.append(tweet)\n",
    "\n",
    "irr_lower = [] #Colocando as frases em lower case\n",
    "for i in range(0, len(irr_raw)):\n",
    "    irr_lower.append(cleanup(irr_raw[i].lower()))\n",
    "\n",
    "irr = [] #Transformando a lista em um √≠ndice para cada palavra\n",
    "for frase in irr_lower:\n",
    "    x = frase.split()\n",
    "    for i in range(0, len(x)):\n",
    "        irr.append(x[i])\n",
    "\n",
    "#Data Frame\n",
    "irr_serie = pd.Series(irr)\n",
    "irr_relativo = irr_serie.value_counts(True)\n",
    "\n",
    "# ###Transformando os tweets relevantes em DataFrame############################################################################\n",
    "\n",
    "rel_raw = [] #Colocando em uma lista\n",
    "tweets_relevantes = train.loc[train['Relevancia'] == 1, :]\n",
    "\n",
    "for tweet in tweets_relevantes['Treinamento']:\n",
    "    rel_raw.append(tweet)\n",
    "\n",
    "rel_lower = [] #Colocando as frases em lower case\n",
    "for i in range(0, len(rel_raw)):\n",
    "    rel_lower.append(cleanup(rel_raw[i].lower()))\n",
    "\n",
    "rel = [] #Transformando a lista em um √≠ndice para cada palavra\n",
    "for frase in rel_lower:\n",
    "    x = frase.split()\n",
    "    for i in range(0, len(x)):\n",
    "        rel.append(x[i])\n",
    "\n",
    "#Data Frame\n",
    "rel_serie = pd.Series(rel)\n",
    "rel_relativo = rel_serie.value_counts(True)\n",
    "\n",
    "###Transformando os tweets muito relevantes em DataFrame########################################################################\n",
    "\n",
    "mt_rel_raw = [] #Colocando em uma lista\n",
    "tweets_muito_relevantes = train.loc[train['Relevancia'] == 2, :]\n",
    "\n",
    "for tweet in tweets_muito_relevantes['Treinamento']:\n",
    "    mt_rel_raw.append(tweet)\n",
    "\n",
    "mt_rel_lower = [] #Colocando as frases em lower case\n",
    "for i in range(0, len(mt_rel_raw)):\n",
    "    mt_rel_lower.append(cleanup(mt_rel_raw[i].lower()))\n",
    "\n",
    "mt_rel = [] #Transformando a lista em um √≠ndice para cada palavra\n",
    "for frase in mt_rel_lower:\n",
    "    x = frase.split()\n",
    "    for i in range(0, len(x)):\n",
    "        mt_rel.append(x[i])\n",
    "\n",
    "#Data Frame\n",
    "mt_rel_serie = pd.Series(mt_rel)\n",
    "mt_rel_relativo = mt_rel_serie.value_counts(True)\n",
    "\n",
    "###Transformando os tweets muito relevantes em DataFrame########################################################################\n",
    "\n",
    "int_raw = [] #Colocando em uma lista\n",
    "tweets_int = train.loc[train['Relevancia'] == 3, :]\n",
    "\n",
    "for tweet in tweets_int['Treinamento']:\n",
    "    int_raw.append(tweet)\n",
    "\n",
    "int_lower = [] #Colocando as frases em lower case\n",
    "for i in range(0, len(int_raw)):\n",
    "    int_lower.append(cleanup(int_raw[i].lower()))\n",
    "\n",
    "int_ver = [] #Transformando a lista em um √≠ndice para cada palavra\n",
    "for frase in int_lower:\n",
    "    x = frase.split()\n",
    "    for i in range(0, len(x)):\n",
    "        int_ver.append(x[i])\n",
    "\n",
    "#Data Frame\n",
    "int_serie = pd.Series(mt_rel)\n",
    "int_relativo = int_serie.value_counts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "###C√°lculo de P(I), P(R) e P(MR)################################################################################################\n",
    "\n",
    "probI = len(irr_serie)/len(total_serie)\n",
    "probR = len(rel_serie)/len(total_serie)\n",
    "probMR = len(mt_rel_serie)/len(total_serie)\n",
    "probIV = len(int_serie)/len(total_serie)\n",
    "\n",
    "###C√°lculo da probabilidade#####################################################################################################\n",
    "\n",
    "def Prob(tweet):\n",
    "    twt = cleanup(tweet.lower()) #Limpar o tweet\n",
    "    tweet_certo = twt.split()\n",
    "    \n",
    "    ProbTweetDadoI = 1\n",
    "    ProbTweetDadoR = 1\n",
    "    ProbTweetDadoMR = 1\n",
    "    ProbTweetDadoIV = 1\n",
    "    \n",
    "  #C√°lculo de P(tweet|I), P(tweet|R) e P(tweet|MR)##############################################################################\n",
    "    \n",
    "    for palavra in tweet_certo:\n",
    "        \n",
    "        if palavra in irr_serie.value_counts():\n",
    "            cont_palavra_irr = irr_serie.value_counts()[palavra]\n",
    "        else:\n",
    "            cont_palavra_irr = 0\n",
    "            \n",
    "        if palavra in rel_serie.value_counts():\n",
    "            cont_palavra_rel = rel_serie.value_counts()[palavra]\n",
    "        else:\n",
    "            cont_palavra_rel = 0\n",
    "            \n",
    "        if palavra in mt_rel_serie.value_counts():\n",
    "            cont_palavra_mt_rel = mt_rel_serie.value_counts()[palavra]\n",
    "        else:\n",
    "            cont_palavra_mt_rel = 0\n",
    "            \n",
    "        if palavra in int_serie.value_counts():\n",
    "            cont_palavra_int = int_serie.value_counts()[palavra]\n",
    "        else:\n",
    "            cont_palavra_int = 0\n",
    "    \n",
    "        ProbTweetDadoI = ProbTweetDadoI * ((cont_palavra_irr + 1)/(len(irr_serie) + len(total_relativo)))\n",
    "        ProbTweetDadoR = ProbTweetDadoR * ((cont_palavra_rel + 1)/(len(rel_serie) + len(total_relativo)))\n",
    "        ProbTweetDadoMR = ProbTweetDadoMR * ((cont_palavra_mt_rel + 1)/(len(mt_rel_serie) + len(total_relativo)))\n",
    "        ProbTweetDadoIV = ProbTweetDadoIV * ((cont_palavra_int + 1)/(len(int_serie) + len(total_relativo)))\n",
    "\n",
    "  #C√°lculo de P(I|tweet), P(R|tweet) e P(MR|tweet)##############################################################################\n",
    "    P_I_tweet = ProbTweetDadoI * probI\n",
    "    P_R_tweet = ProbTweetDadoR * probR\n",
    "    P_MR_tweet = ProbTweetDadoMR * probMR\n",
    "    P_I_V_tweet = ProbTweetDadoIV * probIV\n",
    "    \n",
    "    lista = [P_I_tweet,P_R_tweet,P_MR_tweet,]\n",
    "    \n",
    "    if P_I_tweet == max(lista):\n",
    "        return 0\n",
    "    elif P_R_tweet == max(lista):\n",
    "        return 1\n",
    "    elif P_MR_tweet == max(lista):\n",
    "        return 2\n",
    "    else:\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Verificando a performance do Classificador\n",
    "\n",
    "Agora voc√™ deve testar o seu classificador com a base de Testes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "lista_relevancia = []\n",
    "for tweet in test['Teste']:\n",
    "    y = Prob(tweet)\n",
    "    lista_relevancia.append(y)\n",
    "    i += 1\n",
    "    \n",
    "lista_relevancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Relevancia</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.327273</td>\n",
       "      <td>0.563636</td>\n",
       "      <td>0.109091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.077922</td>\n",
       "      <td>0.883117</td>\n",
       "      <td>0.038961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.161290</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>0.677419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0              0         1         2\n",
       "Relevancia                              \n",
       "0           0.327273  0.563636  0.109091\n",
       "1           0.077922  0.883117  0.038961\n",
       "2           0.161290  0.161290  0.677419\n",
       "3           0.027027  0.972973  0.000000\n",
       "All         0.150000  0.700000  0.150000"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = np.array(lista_relevancia)\n",
    "pd.crosstab(test['Relevancia'], array, margins = True, normalize = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    186\n",
       "3    133\n",
       "0    127\n",
       "2    104\n",
       "Name: Relevancia, dtype: int64"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Relevancia'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    77\n",
       "0    55\n",
       "3    37\n",
       "2    31\n",
       "Name: Relevancia, dtype: int64"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['Relevancia'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Concluindo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.72727272727273\n",
      "67.27272727272727\n",
      "88.31168831168831\n",
      "11.688311688311687\n",
      "67.74193548387096\n",
      "32.25806451612903\n",
      "0\n",
      "100\n",
      "53.5\n"
     ]
    }
   ],
   "source": [
    "II = 18/55 * 100\n",
    "IO = (55-18)/55 * 100\n",
    "RR = 68/77 * 100\n",
    "RO =(77-68)/77 * 100\n",
    "MRMR = 21/31 * 100\n",
    "MRO = (31-21)/31 * 100\n",
    "IVIV = 0\n",
    "IVO = 100\n",
    "CC = (18+68+21)/200 * 100\n",
    "print(II)\n",
    "print(IO)\n",
    "print(RR)\n",
    "print(RO)\n",
    "print(MRMR)\n",
    "print(MRO)\n",
    "print(IVIV)\n",
    "print(IVO)\n",
    "print(CC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.google.com/document/d/1hguTbaLYGZNkuv1Gurj3sQ17LGswFdFzMzAXKRSiXWo/edit link para a conclus√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Qualidade do Classificador a partir de novas separa√ß√µes dos tweets entre Treinamento e Teste\n",
    "\n",
    "Caso for fazer esse item do Projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrumatreinamento(treinamento):\n",
    "    novo = {'tweets':'Treinamento'}\n",
    "    treinamento = treinamento.rename(columns=novo)\n",
    "    treinamento = treinamento.reset_index()\n",
    "    return treinamento\n",
    "\n",
    "def arrumateste(teste):\n",
    "    novo = {'tweets':'Teste'}\n",
    "    teste = teste.rename(columns=novo)\n",
    "    teste = teste.reset_index()\n",
    "    return teste\n",
    "\n",
    "novonome = {'Treinamento':'tweets','Teste':'tweets'}\n",
    "novodf = pd.concat([train.rename(columns=novonome), test.rename(columns=novonome)],ignore_index = True)\n",
    "novodf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>Relevancia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>o shang chi anos atr√°s ser8a interpretando pel...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>assisti shang chi no come√ßo pensei  caramba qu...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@ifyouonlysayyes shang chi old que bom que n√£o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@kempiroto c√™ sabe se shang chi j√° saiu no trr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>shang chi √© bom p caralho o lance da rela√ß√£o f...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>preciso tomar vergonha na cara e assistir shan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets  Relevancia\n",
       "0  o shang chi anos atr√°s ser8a interpretando pel...           0\n",
       "1  assisti shang chi no come√ßo pensei  caramba qu...           2\n",
       "2  @ifyouonlysayyes shang chi old que bom que n√£o...           0\n",
       "3  @kempiroto c√™ sabe se shang chi j√° saiu no trr...           1\n",
       "4  shang chi √© bom p caralho o lance da rela√ß√£o f...           2\n",
       "5  preciso tomar vergonha na cara e assistir shan...           1"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "novodf[\"tweets\"].str.lower()\n",
    "novodf[\"tweets\"] = novodf[\"tweets\"].str.replace('[,!-.:?;]', '')\n",
    "\n",
    "novodf.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista100 = []\n",
    "for p in range(0,99):\n",
    "    lista_relevancia2 = []\n",
    "    train, test = train_test_split(novodf, test_size=0.26666)\n",
    "    train2 = arrumatreinamento(train)\n",
    "    test2 = arrumateste(test)\n",
    "    \n",
    "    frases_lower2 = train2['Treinamento'].tolist() #Colocando frases em uma lista\n",
    "    \n",
    "    total2 = [] #Transformando a lista em um √≠ndice para cada palavra\n",
    "    for frase in frases_lower2:\n",
    "        x = frase.split()\n",
    "        for i in range(0, len(x)):\n",
    "            total2.append(x[i])\n",
    "\n",
    "    total_serie2 = pd.Series(total2)\n",
    "    total_relativo2 = total_serie2.value_counts(True)\n",
    "\n",
    "\n",
    "    tweets_irrelevantes2 = train2.loc[train2['Relevancia'] == 0, :]\n",
    "    irr_lower2 = tweets_irrelevantes2['Treinamento'].tolist() #Colocando em uma lista\n",
    "    \n",
    "\n",
    "    irr2 = [] #Transformando a lista em um √≠ndice para cada palavra\n",
    "    for frases in irr_lower2:\n",
    "        x = frases.split()\n",
    "        for d in range(0, len(x)):\n",
    "            irr2.append(x[d])\n",
    "\n",
    "    irr_serie2 = pd.Series(irr2)\n",
    "    irr_relativo2 = irr_serie2.value_counts(True)\n",
    "\n",
    "    \n",
    "    tweets_relevantes2 = train2.loc[train2['Relevancia'] == 1, :]\n",
    "    rel_lower2 = tweets_relevantes2['Treinamento'].tolist() #Colocando em uma lista\n",
    "    \n",
    "    rel2 = [] #Transformando a lista em um √≠ndice para cada palavra\n",
    "    for frases2 in rel_lower2:\n",
    "        x = frases2.split()\n",
    "        for e in range(0, len(x)):\n",
    "            rel2.append(x[e])\n",
    "\n",
    "    rel_serie2 = pd.Series(rel2)\n",
    "    rel_relativo2 = rel_serie2.value_counts(True)\n",
    "\n",
    "    \n",
    "    tweets_muito_relevantes2 = train2.loc[train2['Relevancia'] == 2, :]\n",
    "    mt_rel_lower2 = tweets_muito_relevantes2['Treinamento'].tolist()  #Colocando em uma lista\n",
    "\n",
    "\n",
    "    mt_rel2 = [] #Transformando a lista em um √≠ndice para cada palavra\n",
    "    for frases3 in mt_rel_lower2:\n",
    "        x = frases3.split()\n",
    "        for g in range(0, len(x)):\n",
    "            mt_rel2.append(x[g])\n",
    "\n",
    "    mt_rel_serie2 = pd.Series(mt_rel2)\n",
    "    mt_rel_relativo2 = mt_rel_serie2.value_counts(True)\n",
    "\n",
    "    \n",
    "    tweets_int2 = train2.loc[train2['Relevancia'] == 3, :]\n",
    "    int_lower2 = tweets_int2['Treinamento'].tolist() #Colocando em uma lista\n",
    "    \n",
    "    int_ver2 = [] #Transformando a lista em um √≠ndice para cada palavra\n",
    "    for frases4 in int_lower2:\n",
    "        x = frases4.split()\n",
    "        for j in range(0, len(x)):\n",
    "            int_ver2.append(x[j])\n",
    "\n",
    "    int_serie2 = pd.Series(mt_rel2)\n",
    "    int_relativo2 = int_serie2.value_counts(True)\n",
    "    probI2 = len(irr_serie2)/len(total_serie2)\n",
    "    probR2 = len(rel_serie2)/len(total_serie2)\n",
    "    probMR2 = len(mt_rel_serie2)/len(total_serie2)\n",
    "    probIV2 = len(int_serie2)/len(total_serie2)\n",
    "\n",
    "    \n",
    "    for tweets in test2['Teste']:\n",
    "        y2 = Prob(tweets)\n",
    "        lista_relevancia2.append(y2)\n",
    "    \n",
    "    array2 = np.array(lista_relevancia2)\n",
    "    tabela = pd.DataFrame\n",
    "    tabela = pd.crosstab(test2['Relevancia'], array2, margins = True, normalize = 'index') #erro\n",
    "    contagem = test2['Relevancia'].value_counts()\n",
    "    CC2 = (contagem[0]*tabela[0][0] + contagem[1]*tabela[1][1] + contagem[2]*tabela[2][2])/200 * 100\n",
    "    lista100.append(CC2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[56.00000000000001,\n",
       " 49.5,\n",
       " 47.0,\n",
       " 52.0,\n",
       " 45.5,\n",
       " 49.0,\n",
       " 47.0,\n",
       " 52.0,\n",
       " 54.0,\n",
       " 46.0,\n",
       " 49.0,\n",
       " 51.5,\n",
       " 52.5,\n",
       " 49.0,\n",
       " 46.0,\n",
       " 46.5,\n",
       " 50.5,\n",
       " 50.0,\n",
       " 44.5,\n",
       " 50.5,\n",
       " 51.0,\n",
       " 47.0,\n",
       " 48.0,\n",
       " 46.5,\n",
       " 52.0,\n",
       " 48.5,\n",
       " 46.0,\n",
       " 42.5,\n",
       " 54.0,\n",
       " 48.0,\n",
       " 50.0,\n",
       " 48.5,\n",
       " 48.0,\n",
       " 46.0,\n",
       " 52.0,\n",
       " 50.5,\n",
       " 44.0,\n",
       " 51.5,\n",
       " 49.5,\n",
       " 50.0,\n",
       " 47.0,\n",
       " 50.0,\n",
       " 48.0,\n",
       " 52.0,\n",
       " 45.0,\n",
       " 48.0,\n",
       " 47.5,\n",
       " 51.5,\n",
       " 48.0,\n",
       " 50.5,\n",
       " 52.0,\n",
       " 46.0,\n",
       " 48.5,\n",
       " 46.0,\n",
       " 54.0,\n",
       " 48.0,\n",
       " 51.5,\n",
       " 48.0,\n",
       " 52.5,\n",
       " 50.0,\n",
       " 43.0,\n",
       " 52.0,\n",
       " 47.5,\n",
       " 47.5,\n",
       " 51.0,\n",
       " 52.5,\n",
       " 47.5,\n",
       " 49.0,\n",
       " 48.5,\n",
       " 46.5,\n",
       " 46.0,\n",
       " 51.0,\n",
       " 51.5,\n",
       " 53.0,\n",
       " 49.0,\n",
       " 51.0,\n",
       " 53.5,\n",
       " 54.50000000000001,\n",
       " 48.0,\n",
       " 53.0,\n",
       " 48.5,\n",
       " 56.00000000000001,\n",
       " 51.0,\n",
       " 51.5,\n",
       " 51.5,\n",
       " 49.5,\n",
       " 52.5,\n",
       " 52.5,\n",
       " 50.0,\n",
       " 56.00000000000001,\n",
       " 46.0,\n",
       " 47.5,\n",
       " 49.5,\n",
       " 51.5,\n",
       " 49.0,\n",
       " 52.0,\n",
       " 47.0,\n",
       " 48.0,\n",
       " 50.5]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAW3klEQVR4nO3df5BdZ33f8fcnchzCD8Wi3mAjCSSCiEelFFwhnNKkJA61ZBxEO00rd4xTN0WjDMY4E4YYmCk0M50y4FCbqcca1YiJYyduMZCqjIpwydBMGuRobQxEFi4bgdAiGy+1sUAYxKJv/7iHcHN1V3sk3dVeH79fMzt7z3me89zvvdrz2bNH5z4nVYUkqbt+YrELkCQtLINekjrOoJekjjPoJanjDHpJ6jiDXpI6zqCXpI4z6KUzkOQ9Se5Y7DqkkzHopdOU5JzFrkFqw6BXZyW5IclfJ/l2kgeT/NO+tjcl2d/XdnGz/vlJPppkJslXklzXt817ktyd5I4kR4CtwDuBf5nkO0k+3zfGziSPJZlK8qa+MdYnmUxyJMk3knzgrL0hetryiERd9tfALwKPAL8O3JHkxcA/At4DvAGYBH4O+EGSnwD+B/DfgSuBFcD/SvJQVe1uxtzUjHU18FPA+cCLq+qqvuf9Y2Af8HzgIuCeJAeq6tPAzcDNVfWHSZ4NvHSBXrv0NzyiV2dV1Ueq6nBVHa+q/wp8GVgP/FvgfVW1t3qmquog8Epgoqp+r6qOVdUB4L8Am/uG/WxV/Ukz5pODz5lkJb1fJL9bVd+rqgeA24A3Nl1+ALw4yflV9Z2q2rNQr1/6EYNenZXk6iQPJPlWkm/RO3o+H1hJ72h/0AuB5/+of7PNO4Hn9fU5NM/TPh94rKq+3bfuILC8efybwEuALyXZm+SKU31d0qny1I06KckL6R2NX0rvKPyHSR4AQi+sf27IZoeAr1TVmpMMPTjd6+DyYeC5SZ7TF/YvAL4OUFVfBq5sThP9M+DuJH+nqo62f3XSqfGIXl31LHohPAOQ5Bp+fD78NuBtSf5Bel7c/GL4S+BIkt9N8tNJliR5aZJXnuR5vgGsaoKbqjoE/AXwH5M8I8nL6B3F39nUcVWSiao6DnyrGeOHo3zh0iCDXp1UVQ8Cvw98ll4Y/z3g/zRtHwH+A/BHwLeBPwGeW1U/BH4NeDnwFeCb9H4p/MxJnuojzff/l+T+5vGVwCp6R/cfB95dVfc0bRuAfUm+Q+8/ZjdX1ffO7NVKJxdvPCJJ3eYRvSR1nEEvSR1n0EtSxxn0ktRxY3kd/fnnn1+rVq1a7DIk6Snjvvvu+2ZVTQxrG8ugX7VqFZOTk4tdhiQ9ZSQ5OFebp24kqeMMeknqOINekjrOoJekjjPoJanjDHpJ6jiDXpI6zqCXpI4z6CWp48byk7HSfB5/HI4cGd14S5fCsmWjG08aJwa9npKOHIHdu0c33mWXGfTqLk/dSFLHGfSS1HEGvSR1nEEvSR1n0EtSx7UK+iQbkjyUZCrJDUPaL0ry2STfT/K2gbbzktyd5EtJ9if5hVEVL0ma37yXVyZZAtwCvBaYBvYm2VlVD/Z1ewy4DnjDkCFuBj5ZVf88ybnAM8+4aklSa22O6NcDU1V1oKqOAXcBm/o7VNWjVbUX+EH/+iRLgV8CPtT0O1ZV3xpF4ZKkdtoE/XLgUN/ydLOujRcBM8CHk3wuyW1JnjWsY5ItSSaTTM7MzLQcXpI0nzZBnyHrquX45wAXA7dW1SuAo8AJ5/gBqmp7Va2rqnUTE0NvZC5JOg1tgn4aWNm3vAI43HL8aWC6qu5tlu+mF/ySpLOkTdDvBdYkWd38Z+pmYGebwavqEeBQkp9vVl0KPHiSTSRJIzbvVTdVNZvkWmA3sATYUVX7kmxt2rcluQCYBJYCx5NcD6ytqiPAW4A7m18SB4BrFualSJKGaTV7ZVXtAnYNrNvW9/gReqd0hm37ALDu9EuUJJ0JPxkrSR1n0EtSxxn0ktRxBr0kdZxBL0kdZ9BLUscZ9JLUcQa9JHWcQS9JHWfQS1LHGfSS1HGt5rqRtLgefxyOHBntmEuXwrJlox1T48mgl54CjhyB3btHO+Zllxn0TxeeupGkjjPoJanjWgV9kg1JHkoyleSEe74muSjJZ5N8P8nbhrQvaW4O/olRFC1Jam/eoE+yBLgF2AisBa5Msnag22PAdcCNcwzzVmD/GdQpSTpNbY7o1wNTVXWgqo4BdwGb+jtU1aNVtRf4weDGSVYArwNuG0G9kqRT1CbolwOH+panm3Vt3QS8HTh+CttIkkakTdBnyLpqM3iSK4BHq+q+Fn23JJlMMjkzM9NmeElSC22CfhpY2be8AjjccvxXA69P8lV6p3x+JckdwzpW1faqWldV6yYmJloOL0maT5ug3wusSbI6ybnAZmBnm8Gr6h1VtaKqVjXb/WlVXXXa1UqSTtm8n4ytqtkk1wK7gSXAjqral2Rr074tyQXAJLAUOJ7kemBtVY34Q9uSpFPVagqEqtoF7BpYt63v8SP0TumcbIzPAJ855QolSWfET8ZKUscZ9JLUcQa9JHWcQS9JHWfQS1LHGfSS1HEGvSR1nEEvSR1n0EtSxxn0ktRxBr0kdZxBL0kdZ9BLUscZ9JLUcQa9JHVcq/noJZ2axx+HIyO87c6TT45uLD39tAr6JBuAm+ndYeq2qnrvQPtFwIeBi4F3VdWNzfqVwO3ABcBxYHtV3Ty68qXxdOQI7N49uvEuuWR0Y+npZ96gT7IEuAV4Lb0bhe9NsrOqHuzr9hhwHfCGgc1ngd+pqvuTPAe4L8k9A9tKkhZQm3P064GpqjpQVceAu4BN/R2q6tGq2gv8YGD9w1V1f/P428B+YPlIKpcktdIm6JcDh/qWpzmNsE6yCngFcO8c7VuSTCaZnJmZOdXhJUlzaBP0GbKuTuVJkjwb+ChwfVUN/S+qqtpeVeuqat3ExMSpDC9JOok2QT8NrOxbXgEcbvsESX6SXsjfWVUfO7XyJElnqk3Q7wXWJFmd5FxgM7CzzeBJAnwI2F9VHzj9MiVJp2veq26qajbJtcBuepdX7qiqfUm2Nu3bklwATAJLgeNJrgfWAi8D3gh8MckDzZDvrKpdI38lkqShWl1H3wTzroF12/oeP0LvlM6gP2f4OX5J0lniFAiS1HEGvSR1nEEvSR1n0EtSxzl7pRbcqGdyhNHP5jg7CwcPjm48Z5vUODHoteBGPZMjjH42x6NHYc+e0Y3nbJMaJ566kaSOM+glqeMMeknqOINekjrOoJekjjPoJanjDHpJ6jiDXpI6zqCXpI4z6CWp41oFfZINSR5KMpXkhiHtFyX5bJLvJ3nbqWwrSVpY8wZ9kiXALcBGercHvDLJ2oFujwHXATeexraSpAXUZlKz9cBUVR0ASHIXsAl48EcdqupR4NEkrzvVbXXmRj075NKlsGzZ6MaTtLjaBP1y4FDf8jTwqpbjt942yRZgC8ALXvCClsMLRj875GWXGfRSl7Q5Rz/s5t7VcvzW21bV9qpaV1XrJiYmWg4vSZpPm6CfBlb2La8ADrcc/0y2lSSNQJug3wusSbI6ybnAZmBny/HPZFtJ0gjMe46+qmaTXAvsBpYAO6pqX5KtTfu2JBcAk8BS4HiS64G1VXVk2LYL9FokSUO0upVgVe0Cdg2s29b3+BF6p2VabStJOnv8ZKwkdZxBL0kdZ9BLUscZ9JLUcQa9JHWcQS9JHdfq8kpJ3TM7CwcPjm48J8MbXwa99DR19Cjs2TO68ZwMb3x56kaSOs6gl6SOM+glqeMMeknqOINekjrOoJekjjPoJanjWgV9kg1JHkoyleSGIe1J8sGm/QtJLu5r++0k+5L8VZI/TvKMUb4ASdLJzRv0SZYAtwAbgbXAlUnWDnTbCKxpvrYAtzbbLgeuA9ZV1Uvp3WVq88iqlyTNq80R/XpgqqoOVNUx4C5g00CfTcDt1bMHOC/JhU3bOcBPJzkHeCbeHFySzqo2Qb8cONS3PN2sm7dPVX0duBH4GvAw8ERVfWrYkyTZkmQyyeTMzEzb+iVJ82gT9Bmyrtr0SbKM3tH+auD5wLOSXDXsSapqe1Wtq6p1ExMTLcqSJLXRZlKzaWBl3/IKTjz9MlefXwW+UlUzAEk+BvxD4I7TLVjSeHI2zPHVJuj3AmuSrAa+Tu8/U//VQJ+dwLVJ7gJeRe8UzcNJvgZckuSZwJPApcDkyKqXNDacDXN8zRv0VTWb5FpgN72rZnZU1b4kW5v2bcAu4HJgCvgucE3Tdm+Su4H7gVngc8D2hXghkqThWs1HX1W76IV5/7ptfY8LePMc274bePcZ1ChJOgN+MlaSOs6gl6SOM+glqeMMeknqOINekjrOoJekjjPoJanjDHpJ6jiDXpI6zqCXpI4z6CWp4wx6Seo4g16SOs6gl6SOM+glqeMMeknquFZBn2RDkoeSTCW5YUh7knywaf9Ckov72s5LcneSLyXZn+QXRvkCJEknN2/QJ1kC3AJsBNYCVyZZO9BtI7Cm+doC3NrXdjPwyaq6CPj7wP4R1C1JaqnNEf16YKqqDlTVMeAuYNNAn03A7dWzBzgvyYVJlgK/BHwIoKqOVdW3Rle+JGk+bYJ+OXCob3m6Wdemz4uAGeDDST6X5LYkzxr2JEm2JJlMMjkzM9P6BUiSTq5N0GfIumrZ5xzgYuDWqnoFcBQ44Rw/QFVtr6p1VbVuYmKiRVmSpDbaBP00sLJveQVwuGWfaWC6qu5t1t9NL/glSWdJm6DfC6xJsjrJucBmYOdAn53A1c3VN5cAT1TVw1X1CHAoyc83/S4FHhxV8ZKk+Z0zX4eqmk1yLbAbWALsqKp9SbY27duAXcDlwBTwXeCaviHeAtzZ/JI4MNAmSVpg8wY9QFXtohfm/eu29T0u4M1zbPsAsO70S5QknQk/GStJHWfQS1LHGfSS1HEGvSR1nEEvSR1n0EtSxxn0ktRxBr0kdZxBL0kdZ9BLUscZ9JLUcQa9JHWcQS9JHddq9kpJOttmZ+HgwdGOuXQpLFs22jGfCgx6SWPp6FHYs2e0Y1522dMz6FudukmyIclDSaaSnHDP1+bOUh9s2r+Q5OKB9iXNzcE/MarCJUntzBv0SZYAtwAbgbXAlUnWDnTbCKxpvrYAtw60vxXYf8bVSpJOWZsj+vXAVFUdqKpjwF3ApoE+m4Dbq2cPcF6SCwGSrABeB9w2wrolSS21CfrlwKG+5elmXds+NwFvB46fXomSpDPRJugzZF216ZPkCuDRqrpv3idJtiSZTDI5MzPToixJUhttgn4aWNm3vAI43LLPq4HXJ/kqvVM+v5LkjmFPUlXbq2pdVa2bmJhoWb4kaT5tgn4vsCbJ6iTnApuBnQN9dgJXN1ffXAI8UVUPV9U7qmpFVa1qtvvTqrpqlC9AknRy815HX1WzSa4FdgNLgB1VtS/J1qZ9G7ALuByYAr4LXLNwJUuSTkWrD0xV1S56Yd6/blvf4wLePM8YnwE+c8oVSpLOiHPdSFLHGfSS1HHOdaMTjHoyqSefHN1Y0jh5/HE4cmR04y3UpGsGvU4w6smkLrlkdGNJ4+TIEdi9e3TjLdSka566kaSOM+glqeMMeknqOINekjrOoJekjjPoJanjDHpJ6jiDXpI6zqCXpI4z6CWp4wx6Seo4g16SOq7VpGZJNgA307vD1G1V9d6B9jTtl9O7w9S/rqr7k6wEbgcuAI4D26vq5hHWf4Jxn01u1PWBs0NKOrl5gz7JEuAW4LX0bgK+N8nOqnqwr9tGYE3z9Srg1ub7LPA7Teg/B7gvyT0D247UuM8mN+r6wNkhJZ1cm1M364GpqjpQVceAu4BNA302AbdXzx7gvCQXNjcIvx+gqr4N7AeWj7B+SdI82gT9cuBQ3/I0J4b1vH2SrAJeAdw77EmSbEkymWRyZmamRVmSpDbaBH2GrKtT6ZPk2cBHgeuraugZ6qraXlXrqmrdxMREi7IkSW20CfppYGXf8grgcNs+SX6SXsjfWVUfO/1SJUmno03Q7wXWJFmd5FxgM7BzoM9O4Or0XAI8UVUPN1fjfAjYX1UfGGnlkqRW5r3qpqpmk1wL7KZ3eeWOqtqXZGvTvg3YRe/Syil6l1de02z+auCNwBeTPNCse2dV7Rrpq5AkzanVdfRNMO8aWLet73EBbx6y3Z8z/Py9JOks8ZOxktRxBr0kdZxBL0kdZ9BLUscZ9JLUca2uunk6m52FgwdHN54zTUqL5+m6Pxv08zh6FPbsGd14zjQpLZ6n6/7sqRtJ6jiDXpI6zqCXpI4z6CWp4wx6Seo4g16SOs6gl6SOM+glqeMMeknquFZBn2RDkoeSTCW5YUh7knywaf9CkovbbitJWljzBn2SJcAtwEZgLXBlkrUD3TYCa5qvLcCtp7CtJGkBtTmiXw9MVdWBqjoG3AVsGuizCbi9evYA5yW5sOW2kqQF1GZSs+XAob7laeBVLfosb7ktAEm20PtrAOA7SR7qaz4f+GaLWseJNZ8d1nx2WPPZcSY1v3CuhjZBP+zm3tWyT5tteyurtgPbhxaQTFbVupMVOW6s+eyw5rPDms+Ohaq5TdBPAyv7llcAh1v2ObfFtpKkBdTmHP1eYE2S1UnOBTYDOwf67ASubq6+uQR4oqoebrmtJGkBzXtEX1WzSa4FdgNLgB1VtS/J1qZ9G7ALuByYAr4LXHOybU+jzqGndMacNZ8d1nx2WPPZsSA1p2roKXNJUkf4yVhJ6jiDXpI6biyDPsmSJJ9L8olm+f1JvtRMr/DxJOctcoknGKy5b/3bklSS8xertrkMqznJW5opK/Yled9i1jfMkJ+NlyfZk+SBJJNJ1i92jf2SfDXJF39UX7PuuUnuSfLl5vuyxa6z3xw1PxX2wRPq7msby/1wrppHvR+OZdADbwX29y3fA7y0ql4G/F/gHYtS1ckN1kySlcBrga8tSkXz+1s1J/llep9cfllV/V3gxsUq7CQG3+f3Af++ql4O/Ltmedz8clW9vO/66BuAT1fVGuDTzfK4Gaz5qbAPwol1PxX2w79V80Lsh2MX9ElWAK8DbvvRuqr6VFXNNot76F2PPzaG1dz4T8DbmeNDYotpjpp/C3hvVX0foKoeXYza5jJHzQUsbR7/DE+Nz2lsAv6gefwHwBsWr5R2xn0fnMfY7odzGPl+OHZBD9xE7x/l+Bzt/wb4n2etmnZuYqDmJK8Hvl5Vn1+souZxEye+zy8BfjHJvUn+d5JXLkplc7uJE2u+Hnh/kkP0jnzG7UizgE8lua+Z5gPgec3nTGi+/+yiVTfcsJr7jeM+CEPqfgrsh8Pe65Hvh20+GXvWJLkCeLSq7kvymiHt7wJmgTvPcmlzGlZzkmcC7wL+ySKWNqeTvM/nAMuAS4BXAv8tyYtqDK7BPUnNvwX8dlV9NMm/AD4E/OoilDiXV1fV4SQ/C9yT5EuLXVALJ9RcVX8G47kP9hn2Xo/tftgYVvPI98OxCnrg1cDrk1wOPANYmuSOqroqyW8AVwCXjkPw9DmhZuAPgdXA55NA78/c+5Osr6pHFq3SHxv6PtObyuJjzfv7l0mO05tkaWbxSv0bc9X8a/TO2wN8hBNPny2qqjrcfH80ycfpzej6jSQXVtXD6c3yOlanyOao+c/GeB8Ehtb9jxnv/XCu93r0+2FVjeUX8BrgE83jDcCDwMRi19W25oH1XwXOX+z6WrzPW4Hfax6/hN7Mo1nsGuepeT/wmubxpcB9i11fX53PAp7T9/gvmp/l9wM3NOtvAN632LW2qHms98G56h7oM1b74Une65Hvh+N2RD+X/wz8FL0/bQD2VNXWxS2pk3YAO5L8FXAM+I1qftrG2JuAm5OcA3yPH091PQ6eB3y8+Zk9B/ijqvpkkr30/hz/TXpXgvz6ItY4aK6apxjvfXBo3Ytb0rzmeq/PZcT7oVMgSFLHjeNVN5KkETLoJanjDHpJ6jiDXpI6zqCXpI4z6CWp4wx6Seq4/w/PsoxsteVEDAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(lista100, bins=15, color='blue', edgecolor='white', density=True, alpha=0.4)\n",
    "plt.title('acertos')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Aperfei√ßoamento:\n",
    "\n",
    "Trabalhos que conseguirem pelo menos conceito B v√£o evoluir em conceito dependendo da quantidade de itens avan√ßados:\n",
    "\n",
    "* IMPLEMENTOU outras limpezas e transforma√ß√µes que n√£o afetem a qualidade da informa√ß√£o contida nos tweets. Ex: stemming, lemmatization, stopwords\n",
    "* CORRIGIU separa√ß√£o de espa√ßos entre palavras e emojis ou entre emojis e emojis\n",
    "* CRIOU categorias intermedi√°rias de relev√¢ncia baseadas na probabilidade: ex.: muito relevante, relevante, neutro, irrelevante, muito irrelevante. Pelo menos quatro categorias, com adi√ß√£o de mais tweets na base, conforme enunciado. (OBRIGAT√ìRIO PARA TRIOS, sem contar como item avan√ßado)\n",
    "* EXPLICOU porqu√™ n√£o pode usar o pr√≥prio classificador para gerar mais amostras de treinamento\n",
    "* PROP√îS diferentes cen√°rios para Na√Øve Bayes fora do contexto do projeto\n",
    "* SUGERIU e EXPLICOU melhorias reais com indica√ß√µes concretas de como implementar (indicar como fazer e indicar material de pesquisa)\n",
    "* FEZ o item 6. Qualidade do Classificador a partir de novas separa√ß√µes dos tweets entre Treinamento e Teste descrito no enunciado do projeto (OBRIGAT√ìRIO para conceitos A ou A+)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Refer√™ncias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Naive Bayes and Text Classification](https://arxiv.org/pdf/1410.5329.pdf)  **Mais completo**\n",
    "\n",
    "[A practical explanation of a Naive Bayes Classifier](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/) **Mais simples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
